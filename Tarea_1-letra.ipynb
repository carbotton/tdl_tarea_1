{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 1 - Taller de Deep Learning\n",
    "\n",
    "**Fecha de entrega: 19/10/2025**  \n",
    "**Puntaje máximo: 15**\n",
    "\n",
    "## Introducción\n",
    "\n",
    "El objetivo de esta tarea es evaluar su conocimiento sobre Deep Learning aplicado a un caso de uso real. En particular, vamos a evaluar la performance de sus modelos en una tarea de clasificación de escenas.\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "El dataset a ser utilizado es **Imagenette**, una versión simplificada de ImageNet que consiste de 10 clases. Pueden encontrar más información y descargarlo en el siguiente [link](https://pytorch.org/vision/main/generated/torchvision.datasets.Imagenette.html).\n",
    "\n",
    "**Tarea**\n",
    "\n",
    "Tienen total libertad sobre cómo implementar y resolver el problema así como las técnicas y herramientas que quieran usar. Recomendamos usar Colab por simplicidad, pero pueden implementarlo en sus máquinas si así lo prefieren. La única limitante es que esperamos que la entrega sea en formato .ipynb (Jupyter Notebook) **con las celdas ya ejecutadas**.\n",
    "\n",
    "**Restricciones**\n",
    "\n",
    "- No se permite utilizar models pre-entrenados (cada modelo debe ser implementado desde cero)\n",
    "- Se requiere que utilicen al menos 2 técnicas de **regularización** (Dropout, BatchNorm, Data Augmentation, etc.)\n",
    "- Se requiere realizar un **análisis de los datos**, que incluya el balanceo de clases y tomar decisiones en base al análisis realizado.\n",
    "- Las decisiones tomadas sobre el preprocesamiento de las imágenes (transforms, augmentation, etc.) deben ser resultado de la exploración del dataset y estar propiamente justificadas (una sección de exploración en el notebook con comentarios es suficiente).\n",
    "\n",
    "**Reporte**\n",
    "\n",
    "En particular, les pedimos que reporten: accuracy, precision, recall y f1.\n",
    "También se espera poder observar la evolución del modelo (en los datos de train y validación) a medida que se entrena (logs, gráficas, etc).\n",
    "\n",
    "**Evidencia de Experimentos**\n",
    "\n",
    "Además, deben presentar evidencia de correr experimentos usando [Weights & Biases (wandb)](https://wandb.ai/). Esto incluye:\n",
    "- Registros detallados de los experimentos.\n",
    "- Gráficas y logs de entrenamiento.\n",
    "- Comparaciones entre diferentes configuraciones de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import (\n",
    "    train,\n",
    "    model_calassification_report,\n",
    "    show_tensor_image,\n",
    "    show_tensor_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos la semilla para que los resultados sean reproducibles\n",
    "SEED = 34\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando mps\n",
      "Usando 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# definimos el dispositivo que vamos a usar\n",
    "DEVICE = \"cpu\"  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"  # si hay GPU, usamos la GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"  # si no hay GPU, pero hay MPS, usamos MPS\n",
    "elif torch.xpu.is_available():\n",
    "    DEVICE = \"xpu\"  # si no hay GPU, pero hay XPU, usamos XPU\n",
    "\n",
    "print(f\"Usando {DEVICE}\")\n",
    "\n",
    "NUM_WORKERS = 0 # Win y MacOS pueden tener problemas con múltiples workers\n",
    "if sys.platform == 'linux':\n",
    "    NUM_WORKERS = 4  # numero de workers para cargar los datos (depende de cada caso)\n",
    "\n",
    "print(f\"Usando {NUM_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128  # tamaño del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz to data/imagenette2.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.56G/1.56G [02:01<00:00, 12.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/imagenette2.tgz to data\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# transforms = T.Compose(\n",
    "#     [\n",
    "#         T.ToTensor(),              # convierte a tensor y normaliza a [0,1]\n",
    "#         T.Resize((32, 32)),        # redimensiona\n",
    "#     ]\n",
    "# )\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "transforms = T.Compose(\n",
    "    [\n",
    "        T.ToImage(),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.Resize((32,32)),\n",
    "        T.Grayscale(num_output_channels=1),\n",
    "        # T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        # T.RandomHorizontalFlip(p=0.5),\n",
    "        # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "        # T.ToTensor(),\n",
    "        # T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "imagenette_train_dataset = datasets.Imagenette(\n",
    "    DATA_DIR, split=\"train\", download=True, transform=transforms\n",
    ")\n",
    "\n",
    "imagenette_test_dataset = datasets.Imagenette(\n",
    "    DATA_DIR, split=\"val\", download=False, transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imagenes con color\n",
    "imagenette_train_dataset[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases: [('tench', 'Tinca tinca'), ('English springer', 'English springer spaniel'), ('cassette player',), ('chain saw', 'chainsaw'), ('church', 'church building'), ('French horn', 'horn'), ('garbage truck', 'dustcart'), ('gas pump', 'gasoline pump', 'petrol pump', 'island dispenser'), ('golf ball',), ('parachute', 'chute')]\n"
     ]
    }
   ],
   "source": [
    "name_classes = imagenette_train_dataset.classes\n",
    "nclasses = len(name_classes)\n",
    "\n",
    "print(f\"Clases: {name_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenette_train_dataset, imagenette_val_dataset = random_split(\n",
    "    imagenette_train_dataset, [0.8, 0.2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    train_loader = DataLoader(\n",
    "        imagenette_train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        imagenette_val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        imagenette_test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LeNet                                    [128, 10]                 --\n",
       "├─Conv2d: 1-1                            [128, 6, 28, 28]          156\n",
       "├─Tanh: 1-2                              [128, 6, 28, 28]          --\n",
       "├─AvgPool2d: 1-3                         [128, 6, 14, 14]          --\n",
       "├─Conv2d: 1-4                            [128, 16, 10, 10]         2,416\n",
       "├─Tanh: 1-5                              [128, 16, 10, 10]         --\n",
       "├─AvgPool2d: 1-6                         [128, 16, 5, 5]           --\n",
       "├─Conv2d: 1-7                            [128, 120, 1, 1]          48,120\n",
       "├─Tanh: 1-8                              [128, 120, 1, 1]          --\n",
       "├─Flatten: 1-9                           [128, 120]                --\n",
       "├─Linear: 1-10                           [128, 84]                 10,164\n",
       "├─Tanh: 1-11                             [128, 84]                 --\n",
       "├─Linear: 1-12                           [128, 10]                 850\n",
       "==========================================================================================\n",
       "Total params: 61,706\n",
       "Trainable params: 61,706\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 54.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 6.67\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 7.45\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear = nn.Linear(120, 84)\n",
    "        self.output = nn.Linear(84, num_classes)\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tanh(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.tanh(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.tanh(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.tanh(self.linear(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "summary(LeNet(1, 10), input_size=(BATCH_SIZE, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train Loss: 2.21255 | Val Loss: 2.12550\n",
      "Epoch: 002 | Train Loss: 2.08984 | Val Loss: 2.07637\n",
      "Epoch: 003 | Train Loss: 2.05866 | Val Loss: 2.07603\n",
      "Epoch: 004 | Train Loss: 2.04808 | Val Loss: 2.04155\n",
      "Epoch: 005 | Train Loss: 2.02923 | Val Loss: 2.02963\n",
      "Epoch: 006 | Train Loss: 2.01767 | Val Loss: 2.02244\n",
      "Epoch: 007 | Train Loss: 1.99880 | Val Loss: 2.01183\n",
      "Epoch: 008 | Train Loss: 1.98871 | Val Loss: 2.00097\n",
      "Epoch: 009 | Train Loss: 1.96968 | Val Loss: 1.98997\n",
      "Epoch: 010 | Train Loss: 1.94914 | Val Loss: 1.99237\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "EPOCHS = 10\n",
    "\n",
    "letnet_model = LeNet(1,10).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(letnet_model.parameters(), lr=LR)\n",
    "\n",
    "_, _ = train(\n",
    "    letnet_model,\n",
    "    optimizer,\n",
    "    criterion, \n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    DEVICE,\n",
    "    True,\n",
    "    5,\n",
    "    EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploracion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-taller2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
